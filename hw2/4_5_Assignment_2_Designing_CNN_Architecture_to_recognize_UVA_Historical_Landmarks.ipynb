{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZAKdaebB3wx",
        "outputId": "f0ef91af-e162-4566-dbac-3e6c82937314"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v4u4YC8iqxpe"
      },
      "source": [
        "# Assignment 2: Designing an Architecture for Recognizing UVA Historical Landmarks\n",
        "![UVA Grounds](https://sworld.co.uk/img/img/131/photoAlbum/5284/originals/0.jpg)\n",
        "\n",
        "The UVA Grounds is known for its Jeffersonian architecture and place in U.S. history as a model for college and university campuses throughout the country. Throughout its history, the University of Virginia has won praises for its unique Jeffersonian architecture.\n",
        "\n",
        "In this assignment, you will attempt the build an image recognition system to classify different buildlings/landmarks on Grounds. You will earn 100 points for this assignment if you successfully transfer at least 3 existing architectures, plus 10 bonus points if your classifier performance exceeds 94% accuracy.\n",
        "\n",
        "To make it easier for you, some codes have been provided to help you process the data, you may modify it to fit your needs. You must submit the .ipynb file via UVA Canvas with the following format: yourcomputingID_assignment_2.ipynb\n",
        "\n",
        "Best of luck, and have fun!\n",
        "\n",
        "## TA's notes\n",
        "Requirements:\n",
        "- You must be able to implement a complete training/validation loop with a final test. The basic logic is similar to the FashionMNIST in previous tutorials.\n",
        "- You must use 3 models. You could create/train all models from scratch or you can use transfer learning (for example, torchvision.models.resnet18) for one, two, or all of them. You may find CNN-specific architectures (e.g., LeNet, AlexNet, ResNet variants, GoogLeNet, VGG, etc) will be helpful. The most widely used CNN architectures include VGG-16 and ResNet-FPN-X101 (but they may be too big for Google Colab GPUs, so try small variants if that's the case). You could also try state-of-the-art Vision Transformers (ViT) but it's technically not a CNN so the rest of your models must both be CNN. ViTs might also be too big for the Google Colab GPUs. The above-mentioned architectures all have pretrained versions in [torchvision](https://pytorch.org/vision/stable/models.html).\n",
        "- You can use the Sequential model creation instead of the class API, but that can make adding/changing various modules more difficult.\n",
        "- Please split your own validation set. We've done this in previous tutorials also.\n",
        "- Please do NOT change the random `SEED` or the test dataset/dataloader so I can verify your performance.\n",
        "- Please also plot your training and loss curves. This way you and I will both understand the learning process and identify any problem.\n",
        "- Please make your 3 models sufficiently different. For example, it's not enough to add one layer and call that a different model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUboEeI0CH0A"
      },
      "source": [
        "# Load Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bIAunzfaCNAH"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import sklearn\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "from functools import partial\n",
        "import PIL\n",
        "import PIL.Image\n",
        "import torch\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "from torch import optim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyHYkVx5vquV"
      },
      "source": [
        "# Random Seed for Reproduction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_trs9lqgvpgc"
      },
      "outputs": [],
      "source": [
        "from torch import manual_seed as torch_manual_seed\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from torch.cuda import max_memory_allocated, set_device, manual_seed_all\n",
        "from torch.backends import cudnn\n",
        "\n",
        "def setup_seed(seed):\n",
        "    torch_manual_seed(seed)\n",
        "    manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    cudnn.deterministic = True\n",
        "\n",
        "SEED = 42\n",
        "setup_seed(SEED)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whpECRG_B2Nj"
      },
      "source": [
        "# Import Dataset\n",
        "The full dataset is huge (+37GB) with +13K images of 18 classes. So it will take a while to download, extract, and process. To save you time and effort, a subset of the data has been resized and compressed to only 379Mb and stored in a Firebase server. This dataset will be the one you will benchmark for your grade. If you are up for a challenge (and perhaps bonus points), contact the instructor for the full dataset!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4E5kiDN9OFs",
        "outputId": "123a530a-b557-43cc-e415-9289dae4e12a"
      },
      "outputs": [],
      "source": [
        "# Download dataset from FirebaseStorage\n",
        "#!curl -L \"https://firebasestorage.googleapis.com/v0/b/uva-landmark-images.appspot.com/o/dataset.zip?alt=media&token=e1403951-30d6-42b8-ba4e-394af1a2ddb7\" -o \"dataset.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKWszcAd9WJh",
        "outputId": "089248a3-b390-42f0-e538-279215608a24"
      },
      "outputs": [],
      "source": [
        "# Extract content\n",
        "#!unzip -q \"dataset.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "474nVh3m-FrM"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_files\n",
        "\n",
        "data_dir = \"dataset/\"\n",
        "batch_size = 32\n",
        "# IMPORTANT: Depends on what pre-trained model you choose, you will need to change these dimensions accordingly\n",
        "img_height = 150\n",
        "img_width = 150\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eY_Eljk7B3w2",
        "outputId": "e17d9a2b-6ead-4f62-e534-c999ff4272d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before splitting the full dataset into train and test: len(dataset_all)=14286\n",
            "After splitting the full dataset into train and test: len(dataset_train)=11429. len(dataset_test)=2857\n",
            "After splitting the full dataset into train and val: len(dataset_train)=9144. len(dataset_val)=2285\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import DataLoader, random_split\n",
        "from torch import Generator\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets import ImageFolder\n",
        "\n",
        "\n",
        "TEST_RATIO = 0.2\n",
        "BATCH_SIZE = 10\n",
        "\n",
        "# Download and load the training data\n",
        "dataset_all = ImageFolder(\n",
        "    data_dir,\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "size_all = len(dataset_all)\n",
        "print(f'Before splitting the full dataset into train and test: len(dataset_all)={size_all}')\n",
        "\n",
        "\n",
        "size_test = int(size_all * TEST_RATIO)\n",
        "size_train = size_all - size_test\n",
        "\n",
        "dataset_train, dataset_test = random_split(dataset_all, [size_train, size_test], generator=Generator().manual_seed(SEED))\n",
        "print(f'After splitting the full dataset into train and test: len(dataset_train)={len(dataset_train)}. len(dataset_test)={len(dataset_test)}')\n",
        "\n",
        "VAL_RATIO = 0.2\n",
        "size_val = int(size_train * VAL_RATIO)\n",
        "size_train = size_train - size_val\n",
        "\n",
        "dataset_train, dataset_val = random_split(dataset_train, [size_train, size_val], generator=Generator().manual_seed(SEED))\n",
        "print(f'After splitting the full dataset into train and val: len(dataset_train)={len(dataset_train)}. len(dataset_val)={len(dataset_val)}')\n",
        "\n",
        "# NOTE that you must not use the test dataset for model selection\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "loaders = {\n",
        "    'train' : torch.utils.data.DataLoader(dataset_train, \n",
        "                                          batch_size = 10, \n",
        "                                          shuffle = True, \n",
        "                                          num_workers = 1),\n",
        "    \n",
        "    'test'  : torch.utils.data.DataLoader(dataset_test, \n",
        "                                          batch_size = 10, \n",
        "                                          shuffle = True, \n",
        "                                          num_workers = 1),\n",
        "\n",
        "    'val'   : torch.utils.data.DataLoader(dataset_val,\n",
        "                                          batch_size = 10,\n",
        "                                          shuffle = True,\n",
        "                                          num_workers = 1)\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7tg1WteYWKi"
      },
      "source": [
        "# It's your turn: Building a classifier for UVA Landmark Dataset\n",
        "You may design your own architecture AND re-use any of the exising frameworks.\n",
        "\n",
        "Best of luck!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn import Module\n",
        "\n",
        "num_classes = 18"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "kP0-jzrMYioK"
      },
      "outputs": [],
      "source": [
        "# YOUR CODE STARTS HERE. Feel free to modify anything.\n",
        "\n",
        "# Feel free to rename them!\n",
        "class MyResNet(Module):\n",
        "    def __init__(self):\n",
        "        super(MyResNet, self).__init__()\n",
        "        # Load pre-trained ResNet-18 model\n",
        "        self.resnet18 = models.resnet18(weights = True)\n",
        "        \n",
        "        # Since you might want to fine-tune the model, you can freeze the parameters\n",
        "        for param in self.resnet18.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Modify the fully connected layer to match your task\n",
        "        num_ftrs = self.resnet18.fc.in_features\n",
        "        self.resnet18.fc = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.resnet18(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ws3dKtg7ahrG"
      },
      "outputs": [],
      "source": [
        "class MyVGG(Module):\n",
        "    def __init__(self):\n",
        "        super(MyVGG, self).__init__()\n",
        "        # Load pre-trained VGG11 model\n",
        "        self.vgg11 = models.vgg11(weights = True)\n",
        "        \n",
        "        # Since you might want to fine-tune the model, you can freeze the parameters\n",
        "        for param in self.vgg11.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Modify the fully connected layer to match your task\n",
        "        num_ftrs = self.vgg11.classifier[-1].in_features\n",
        "        self.vgg11.classifier[-1] = nn.Linear(num_ftrs, num_classes)\n",
        "\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.vgg11(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PNk6xhWKoWXU"
      },
      "outputs": [],
      "source": [
        "class MyCodedNet(Module):\n",
        "    def __init__(self):\n",
        "        super(MyCodedNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, stride = (2, 2), kernel_size = 3, padding = 1)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.conv2 = nn.Conv2d(64, 64, kernel_size = 3, padding = 1)\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.conv3 = nn.Conv2d(64, 32, kernel_size = 3, padding = 1)\n",
        "        self.bn3 = nn.BatchNorm2d(32)\n",
        "        self.conv4 = nn.Conv2d(32, 16, kernel_size = 3, padding = 1)\n",
        "        self.bn4 = nn.BatchNorm2d(16)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.fc1 = nn.Linear(16 * 6 * 9, 128)\n",
        "        self.bn5 = nn.BatchNorm1d(128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.bn6 = nn.BatchNorm1d(64)\n",
        "        self.fc3 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, X):\n",
        "        x = self.pool(torch.relu(self.bn1(self.conv1(X))))\n",
        "        x = self.pool(torch.relu(self.bn2(self.conv2(X))))\n",
        "        x = self.pool(torch.relu(self.bn3(self.conv3(X))))\n",
        "        x = self.pool(torch.relu(self.bn4(self.conv4(X))))\n",
        "        x = x.view(-1, 16 * 6 * 9)\n",
        "        x = torch.relu(self.bn5(self.fc1(X)))\n",
        "        x = torch.relu(self.bn6(self.fc2(X)))\n",
        "        x = self.fc3(X)\n",
        "        return X\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Implementing CNN 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/anthonychiado/anaconda3/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "my_res_net = MyResNet()\n",
        "\n",
        "from torch import optim\n",
        "optimizer = optim.Adam(my_res_net.parameters(), lr = 0.01)  \n",
        "loss_func = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
            "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
            "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
            "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "num_epochs = 10\n",
        "my_res_net.train()\n",
        "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
        "\n",
        "    for i, data in enumerate(loaders['train'], 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = my_res_net(inputs)\n",
        "        loss = loss_func(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 60 == 0:    # print every 60 mini-batches\n",
        "            print ('Epoch [{}/{}], Loss: {:.4f}' \n",
        "                       .format(epoch + 1, num_epochs, loss.item()))\n",
        "\n",
        "print('Finished Training')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
